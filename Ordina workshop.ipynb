{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License.\n\nVrijwel alle code in deze workshop is gebaseerd op de Azure Machine Learning voorbeelden van Microsoft."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "# Ordina mTech AI Workshop: Train een classificatie model voor afbeeldingen middels Azure Machine Learning\n\nIn deze workshop ga je een machine learning model trainen op resources die in Azure draaien. We gaan stapsgewijs het model trainen en deployen middels dit Azure (Jupyter) notebook.\n\nQua model wat we gaan trainen hebben we het redelijk eenvoudig gehouden. We hebben overwogen en geprobeerd andere datasets te gebruiken, maar het trainen en optuigen van het model duurt dan simpelweg te lang voor een workshop. Wil je hier verder mee, geef dan een gil aan John, Martijn of Roelant voor een extra dataset waar je in de avonduren op los kan gaan.\n\nWe gaan een 'simpel' logistisch regressie model bouwen voor de [MNIST](http://yann.lecun.com/exdb/mnist/) dataset gebruik makende van Azure Machine Learning services. MNIST is een zeer bekende dataset bestaande uit 70.000 zwart/wit plaatjes. Elk plaatje is een afbeelding van een handgeschreven getal tussen de 0 en 9 van 28x28 pixels. Het model wordt een multi-class classificatie model wat van een plaatje van een handgeschreven getal gaat inschatten welk getal tussen 0 en 9 dit is.\n\n## Prerequisites\n\n- Laptop\n- Azure subscription\n- Github account\n\n(Maar als je dit kan lezen, ben je voor alledrie geslaagd)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Intro\n\nVoor de mensen die nog nooit met een Azure notebook of Jupyter notebook hebben gewerkt, kan dit in het begin een beetje gek voelen. Kort gezegd is een Azure/Jupyter notebook een verzameling van tekst en code, waarbij je de code direct op de pagina kan uitvoeren. Verder ontkom je niet aan python als je iets met machine learning wil doen. Voordeel van python is dat het extreem goed leesbaar is.\n\nNaast de code segmenten staat een 'In [ ]' indicator. Een notebook houdt state en historie bij, dus als je de volgende keer terugkeert in je notebook, is de staat zoals je die hebt achtergelaten. De 'In [ ]' indicator is leeg, mits de stap niet eerder is uitgevoerd. Na uitvoeren van de code, wordt de volgorde van uitgevoerde stappen hier in bijgehouden. Deze index wordt elke keer opgehoogd als de stap wordt uitgevoerd. Na 1x uitvoeren staat er dus 'In [1]'. Als de stap nog wordt uitgevoerd staat er een asterisk (een sterretje dus) 'In [\\*]'. Je kunt eventuele volgende stappen wel in de queue zetten, maar deze worden pas uitgevoerd als de lopende stap klaar is."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configuratie\n\nHet Azure notebook heeft enige configuratie nodig. Het gaat daarbij om 4 variabelen:<br>\n**SUBSCRIPTION_ID** = je Azure subscription Id. Je vindt deze in de portal, bijv. onder Cost Management of onder Subscriptions. <br>\n\nDe onderstaande waarden kan je aanpassen, maar kan je ook zo laten staan.<br>\n**RESOURCE_GROUP** = de naam van een resource group. Als deze niet bestaat, wordt deze aangemaakt. <br>\n**WORKSPACE_NAME** = de naam van de op te spinnen workspace. Als deze niet bestaat, wordt deze aangemaakt. <br>\n**WORKSPACE_REGION** = westeurope (Of als je dapper wil zijn, een andere) "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nsubscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"[SUBSCRIPTION_ID_HERE]\")\nresource_group = os.getenv(\"RESOURCE_GROUP\", default=\"Ordina-ML-rg\")\nworkspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"Ordina-ML-service\")\nworkspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"westeurope\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Workspace\nWe gaan eerst controleren of je workspace al bestaat. Voer de code hieronder uit. De code probeert een workspace op te vragen en te exporteren naar een config bestand. Als de workspace al bestaat, ben je daarna klaar met het configuratie deel en kan je door naar het [importeren van de python packages](#Setup-van-de-development-omgeving-van-je-notebook)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\ntry:\n    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n    # write the details of the workspace to a configuration file to the notebook library\n    ws.write_config()\n    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\nexcept:\n    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Bij het uitvoeren van bovenstaande code, kan het zijn dat je meldingen krijgt over authenticatie / credentials. Dit is verwacht en is een kwestie van gewoon doen wat er staat. Het is het (tijdelijk) toegang geven van het Azure notebook op jouw Azure resources.\n\nWe gaan nu een nieuwe Azure ML workspace opspinnen:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Create the workspace using the specified parameters\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      create_resource_group = True,\n                      exist_ok = True)\nws.get_details()\n\n# write the details of the workspace to a configuration file to the notebook library\nws.write_config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ook hier weer een rode balk als je workspace, maar deze zou informational moeten zijn en je wijzen op het feit dat een resource groep is aangemaakt, omdat deze nog niet bestond. Het eind resultaat zou een melding moeten zijn dat je config succesvol is weggeschreven. Als je resource groep wel al bestaat, krijg je uiteraard geen melding.\n\nWe hebben straks ook nog compute resources nodig. Hoewel machine learning modellen uiteraard het beste trainen op GPUs (**<Commercial_Break>** voor als je je afvraagt waarom dat is, Roelant gaat dat proberen uit te leggen tijdens zijn sessie op FutureTech as. woensdag **</Commercial_Break>**) gaan we geen GPU cluster opspinnen. In het originele Microsoft voorbeeld staat de code, dus als je je dapper voelt, voel je vrij, maar in de standaard flow gaan we uit van een CPU cluster waarop we ons model gaan trainen.\n\nVoer nu onderstaande code uit. Het creÃ«ren van het de compute resources kan even duren, dus voel je vrij om anderen te helpen tot deze stap te komen, haal een bak koffie of bespreek je favoriete Netflix serie met iemand die ook net zo ver is."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\n# Choose a name for your CPU cluster\ncpu_cluster_name = \"cpucluster\"\n\n# Verify that cluster does not exist already\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n    print(\"Found existing cpucluster\")\nexcept ComputeTargetException:\n    print(\"Creating new cpucluster\")\n    \n    # Specify the configuration for the new cluster\n    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n                                                           min_nodes=1,\n                                                           max_nodes=4)\n\n    # Create the cluster with the specified name and configuration\n    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n    \n    # Wait for the cluster to complete, show the output log\n    compute_target.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Bovenstaande code geeft feedback over de voortgang van opspinnen van het cluster. We zetten het minimaal aantal draaiende nodes op 1, zodat we daar straks minder lang op hoeven te wachten. Het opspinnen van nieuwe nodes op een 'koude' workspace is vaak een koffie-haal-momentje. Nu duurt het ongeveer 2-3 minuten. \n\nWe gaan nu door met het installeren van pre requisites binnen je Azure notebook."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Setup van de development omgeving van je notebook\n\n### Importeren packages\n\nImporteer met de volgende stap de Python packages die je nodig hebt. Als validatie wordt hier de Azure ML SDK versie als output gegeven. Deze doet er voor de rest van de workshop niet echt toe, maar voor de toekomst kan het handig zijn om problemen met het notebook te detecteren."
    },
    {
      "metadata": {
        "tags": [
          "check version"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Connect naar de workspace\n\nWe gaan een connectie maken naar de zojuist gecreÃ«rde workspace. `Workspace.from_config()` leest de inhoud van het bestand **config.json** en laadt alles in een object `ws`. Dit object zullen we verderop in het notebook gebruiken."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config()\nprint(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Maak een experiment\n\nMaak een experiment aan in je workspace om de voortgang van het trainen van het model te volgen. Zie het als een container waarin je model voortgang te zien is. Een workspace kan meerdere experimenten bevatten, maar we gebruiken er nu maar Ã©Ã©n ('sklearn-mnist'): "
    },
    {
      "metadata": {
        "tags": [
          "create experiment"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'sklearn-mnist'\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We kunnen nu daadwerkelijk je model te gaan trainen. Neem even een moment om stil te staan bij wat we net gedaan hebben. Voor je gevoel heb je een paar keer op run geklikt, maar je hebt gewoon een heel platform gebouwd waarop je machine learning modellen kan trainen en een compute cluster opgespind met bad-ass machines die in de cloud jouw modellen kunnen doorrekenen. Je kan nu al trots zijn!\n\n## MNIST dataset\n\nVoordat we een model kunnen trainen, is het handig als je begrijpt welke data er in de dataset zit en wat het doel is van deze workshop.\n\nWe gaan nu:\n\n* De MNIST dataset downloaden\n* Waat voorbeelden bekijken van plaatjes (van handgeschreven cijfers)\n* De data Azure in laden\n\n### Download de MNIST dataset\n\nOnderstaand code blok gaat de MNIST dataset downloaden en de bestanden naar een `data` map binnen het notebook wegschrijven. Het gaat hierbij om zowel de plaatjes alsook de labels voor testen en trainen van het model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request\n\ndata_folder = os.path.join(os.getcwd(), 'data')\nos.makedirs(data_folder, exist_ok=True)\n\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images.gz'))\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels.gz'))\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'test-images.gz'))\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'test-labels.gz'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Voorbeelden dataset bekijken\n\nDe zips worden in onderstaand code blok in `numpy` arrays geladen. Een `numpy` array is een geÃ¯ndexeerde matrix van waarden van allemaal hetzelfde type. We gebruiken vervolgens de plotting library `matplotlib` om 30 willekeurige plaatjes uit de dataset te tonen inclusief hun label. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from utils import load_data\n\n# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\nX_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\nX_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\ny_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\ny_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n\n# now let's show some randomly chosen images from the traininng set.\ncount = 0\nsample_size = 30\nplt.figure(figsize = (16, 6))\nfor i in np.random.permutation(X_train.shape[0])[:sample_size]:\n    count = count + 1\n    plt.subplot(1, sample_size, count)\n    plt.axhline('')\n    plt.axvline('')\n    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "De voorbeeld plaatjes geven je een idee van de inhoud van de dataset en welk model we gaan trainen. Het idee is dus dat het model straks aan de hand van het plaatje kan gaan voorspellen welk cijfer het is.\n\n### Upload data naar azure\n\nAangezien we in de cloud het model gaan trainen, zal de data ook naar de cloud moeten worden opgeladen zodat de Azure ML workspace hier toegang toe heeft. De datastore in de workspace is hier uitermate geschikt voor. Je kan hier data naar opladen en de compute resources hebben er standaard al toegang toe. Onder water zit hier 'gewoon' een blob storage onder.\n\nOnderstaand code blok laadt de MNIST dataset bestanden in een folder `mnist` in de root van je datastore binnen je Azure ML workspace."
    },
    {
      "metadata": {
        "tags": [
          "use datastore"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\n\nds.upload(src_dir=data_folder, target_path='mnist', overwrite=True, show_progress=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train het model in de cloud\n\nIn een van de voorgaande stappen hebben we een compute cluster opgespind. Op dit cluster gaan we nu een training job starten.\n\n### Folder notebook maken\nWe gaan eerst een folder aanmaken in het notebook die we zullen vullen met code die we naar het compute cluster zullen gaan sturen."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Trainingscript maken\n\nHet trainingscript is waar alle 'magie' gebeurt. Dit is het script wat:\n- de compute nodes binnen het cluster straks uit gaan voeren\n- de train en test data inlaadt\n- een `LogisticRegression` algoritme gebruikt om het model te trainen (`clf.predict`)\n- het model exporteert\n\nDit is dus het stuk waar feitelijk machine learning wordt toegepast. Dit model gebruikt dit specifieke algoritme omdat het sneller te trainen is dan een neuraal netwerk met toch een vergelijkbare nauwkeurigheid. De `regularization rate` waar over gesproken wordt in de comments is om `overfitting` van het model te voorkomen.\n\nMet onderstaand code blok wordt er een file `train.py` weggeschreven in de eerder aangemaakte folder."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport argparse\nimport os\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\n\nfrom azureml.core import Run\nfrom utils import load_data\n\n# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\nargs = parser.parse_args()\n\ndata_folder = args.data_folder\nprint('Data folder:', data_folder)\n\n# load train and test set into numpy arrays\n# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\nX_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\nX_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\ny_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\ny_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n\n# get hold of the current run\nrun = Run.get_context()\n\nprint('Train a logistic regression model with regularization rate of', args.reg)\nclf = LogisticRegression(C=1.0/args.reg, random_state=42)\nclf.fit(X_train, y_train)\n\nprint('Predict the test set')\ny_hat = clf.predict(X_test)\n\n# calculate accuracy on the prediction\nacc = np.average(y_hat == y_test)\nprint('Accuracy is', acc)\n\nrun.log('regularization rate', np.float(args.reg))\nrun.log('accuracy', np.float(acc))\n\nos.makedirs('outputs', exist_ok=True)\n# note file saved in the outputs folder is automatically uploaded into experiment record\njoblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "De `joblib.dump` schrijft het model weg naar de outputs folder. Deze wordt automatisch opgeladen naar het experiment in je Azure ML workspace. Dit gebeurt dus straks binnen het compute cluster als het script klaar is met trainen op het cluster en de output gaat terugschrijven naar het experiment.\n\nOnderstaande code blok kopieert `utils.py` naar de script folder. Dit pyton bestand bevat helpers om de dataset in te laden en moet dus aanwezig zijn voor het compute cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import shutil\nshutil.copy('utils.py', script_folder)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Estimator maken\n\nEen 'estimator' is het object wat het daadwerkelijke werk doet op de compute nodes. Deze heeft nodig:\n\n* De scripts en inhoud van de `sklearn-mnist`-folder. \n* De compute target\n* De naam van het trainingsscrip: `train.py`\n* De training parameters (script_params) \n* De python packages die nodig zijn voor het trainen van het model\n\nDe data_folder wijst hier naar de datastore in je Azure ML Workspace (`ds.path('mnist').as_mount()`)."
    },
    {
      "metadata": {
        "tags": [
          "configure estimator"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.path('mnist').as_mount(),\n    '--regularization': 0.05\n}\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script='train.py',\n                conda_packages=['scikit-learn'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Job naar remote cluster sturen\n\nWe gaan onze eerste job in ons experiment starten door gebruik te maken van de estimator. Eventueel kan je in de Azure portal de voortgang bijhouden van het trainen van het model. In dit geval is de training job normaal gezien relatief snel klaar, maar voor ingewikkeldere modellen is het een goede manier om de voortgang te zien.\n\nVoor nu onderstaand code blok uit om het experiment te starten, maar lees vooral wel verder."
    },
    {
      "metadata": {
        "tags": [
          "remote run",
          "amlcompute",
          "scikit-learn"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(config=est)\nrun",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "(Call is async, dus vertrouw hier niet direct de return status)\n\n## Job monitoren op het cluster\n\nDe eerste run van het model kan ook weer even duren en hangt ook af hoe snel Azure nodes wil opspinnen in je cluster, dus wellicht een goed moment om weer even collega's te polsen over hun voortgang, weer een rondje koffie te doen, of wellicht ben je een aantal termen en concepten tegengekomen die je even op wil zoeken op Google. Vervolg runs van het model zijn vrijwel altijd sneller, omdat er een docker container wordt gemaakt met jouw estimator data die later kan worden hergebruikt.\n\nOm je een beeld te vormen wat er gebeurt terwijl je wacht / koffie haalt:\n\n- **Docker image aanmaken**: Er wordt een docker image gemaakt van je estimator die naar ACR wordt gestuurd. Weet je niet wat ACR is? Nu is het een mooi moment om dit op te zoeken.\n- **Cluster nodes opschalen**: We hebben het compute cluster aangemaakt zonder draaiende nodes. Dit is goedkoper, maar het opspinnen van machines van deze orde van grootte kost nou eenmaal even tijd\n- **Trainen van het model**: Hier gebeurt het 'echte' werk. De scripts worden naar de compute target gestuurd, de data store wordt in je workspace gehangen en het trainingsscript wordt uitgevoerd. \n- **Post-Processing**: Het resultaat wordt teruggeschreven naar je Azure ML workspace\n\n...mocht je het toch lang vinden duren... realiseer dan ook hoeveel tijd dit allemaal kosten voordat we Azure ML workspaces hadden. Er zat altijd een berg handmatig werk in om alle stappen te doorlopen en vaak zat er niks anders op dan te trainen op een data science VM in Azure die je helemaal moest inrichten, of nog erger dat je moest trainen op je lokale machine.\n\n### Jupyter widget\n\nWellicht was je reeds onder de indruk van Jupyter notebooks, maar notebooks kunnen ook widgets bevatten. Hieronder een widget die elke 10-15 seconden ververst en de voortgang toont van je job die draait binnen je experiment."
    },
    {
      "metadata": {
        "tags": [
          "use notebook widget"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Is het al klaar?\n\nNaast de widget en het monitoren in de portal kan je ook 'gewoon' wachten tot het trainen van het model klaar is.\n\nOnderstaand code blok blijft in de 'running' state zolang je model nog met de trainingsrun bezig is."
    },
    {
      "metadata": {
        "tags": [
          "remote run",
          "amlcompute",
          "scikit-learn"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "# specify show_output to True for a verbose log\nrun.wait_for_completion(show_output=False) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### KLAAR! Toon nu het resultaat\n\nGefeliciteerd! Je hebt zojuist je eerste machine learning model getraind. Wees nog trotser dan eerst op jezelf. Misschien voelt het nog niet anders, maar er is een tijd voor en een tijd na het trainen van je eerste ML model.\n\nWe zijn uiteraard wel benieuwd naar de performance en nauwkeurigheid van voorspellen van je zojuist getrainde model:"
    },
    {
      "metadata": {
        "tags": [
          "get metrics"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(run.get_metrics())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Registreer model\n\nDoordat het trainingsscript het model wegschrijft naar de `outputs` folder, wordt het automagisch ge-upload naar het experiment in je workspace. \n\nRegistreer nu het model in de Azure ML workspace zodat je dit model later kan bevragen, onderzoeken of deployen."
    },
    {
      "metadata": {
        "tags": [
          "register model from history"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "# register model \nmodel = run.register_model(model_name='sklearn_mnist', model_path='outputs/sklearn_mnist_model.pkl')\nprint(model.name, model.id, model.version, sep='\\t')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download model\n\nWe gaan het model nu downloaden naar ons notebook:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.download(target_dir=os.getcwd(), exist_ok=True)\n\n# verify the downloaded model file\nfile_path = os.path.join(os.getcwd(), \"sklearn_mnist_model.pkl\")\n\nos.stat(file_path)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Model lokaal testen\n\nVoordat we het model gaan deployen, willen we het eerst lokaal testen.\n\n### Laad test data\n\nLaad de test data uit de **./data/** folder uit je notebook. In een eerdere stap hebben we hier de gedownloade dataset neergezet."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from utils import load_data\nimport os\n\ndata_folder = os.path.join(os.getcwd(), 'data')\n# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster\nX_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\ny_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test data scoren met model\n\nWe gaan nu de test dataset door het model heen halen:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pickle\nfrom sklearn.externals import joblib\n\nclf = joblib.load( os.path.join(os.getcwd(), 'sklearn_mnist_model.pkl'))\ny_hat = clf.predict(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Je krijgt hier overigens weer een rode waarschuwing over versionering van de LogisticRegression. We nemen de waarschuwing ter kennisgeving aan. Dit is soms het nadeel van de notebooks en het niet op detail managen van alle dependencies, maar dat kan je ook net zo hard uitleggen als voordeel. In dit geval schaadt het ons getrainde model niet.\n\n### De confusion matrix\n\nNee... dit is niet een plek waar Neo de weg niet weet. De naam van een confusion matrix is het ingewikkelste eraan. Het is een matrix (Excel is een 2D matrix = een grid) met alle verwachte uitkomsten uitgezet tegen de werkelijke uitkomsten. Idealiter heb je voor elke voorspelde waarde ook de juiste waarde hiervan uit het model teruggekregen. Boven de kolommen en voor de rijen moet je dus de getallen 0-9 denken. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\n\nconf_mx = confusion_matrix(y_test, y_hat)\nprint(conf_mx)\nprint('Overall accuracy:', np.average(y_hat == y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Om het nog meer confusing te maken, kunnen we deze matrix ook visualiseren met `matplotlib`.  De ideale lijn van de visualisatie is dus van linksboven tot rechtsonder. Op deze plekken willen we goede voorspellingen (= hoge zekerheid = donkere vlakken) zien. Hoe hoger de foutmarge, des te lichter de cel is gekleurd. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# normalize the diagonal cells so that they don't overpower the rest of the cells when visualized\nrow_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx / row_sums\nnp.fill_diagonal(norm_conf_mx, 0)\n\nfig = plt.figure(figsize=(8,5))\nax = fig.add_subplot(111)\ncax = ax.matshow(norm_conf_mx, cmap=plt.cm.bone)\nticks = np.arange(0, 10, 1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(ticks)\nax.set_yticklabels(ticks)\nfig.colorbar(cax)\nplt.ylabel('true labels', fontsize=14)\nplt.xlabel('predicted values', fontsize=14)\nplt.savefig('conf.png')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Je ziet dus dat het systeem vaak een 3 voorspelt als het in werkelijkheid een 5 is en ook hier zie je weer de diagonale lijn van linksboven naar rechtsonder.\n\n## Deployment\n\nNu we een werkend en OK scorend model hebben, willen we deze ook gaan consumeren in onze applicaties. We gaan dit doen middels een webservice die we in een Azure Container Instance (ACI) gaan laden. We hebben hiervoor het volgende nodig:\n- Een scoring script wat het model inlaadt en gebruikt om voorspellingen (`model.predict`) uit te voeren \n- Een package dependencies file om de packages die nodig zijn in te laden\n- Een ACI config file\n- Het model\n\nPer conventie staan er in het scoring script twee functies die worden aangeroepen door de web service: `init` en `run`\n\n### Scoring script\nOnderstaand code blok maakt het scoring script aan"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score.py\nimport json\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\nfrom azureml.core.model import Model\n\ndef init():\n    global model\n    # retrieve the path to the model file using the model name\n    model_path = Model.get_model_path('sklearn_mnist')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    # make prediction\n    y_hat = model.predict(data)\n    # you can return any data type as long as it is JSON-serializable\n    return y_hat.tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Package dependencies file\n\nWe schrijven hier de yaml weg met dependencies. Dit is enkel de `scikit-learn` library."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"scikit-learn\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "...mocht je nieuwsgierig zijn hoe zo'n yaml er uitziet, maar voel je ook vrij dit code blok te skippen:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(\"myenv.yml\",\"r\") as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ACI configuratie bestand\n\nWe moeten ACI laten weten welke resources (CPU & RAM) er aangewend mogen worden voor het runnen van de container met het ML model. Het hangt uiteraard geheel af van je model welke resources je hiervoor nodig zal hebben, maar standaard is dat 1 core met 1 Gb en dit is doorgaans voldoende voor de meeste modellen. Je kan uiteraard in een later stadium altijd opschalen:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"data\": \"MNIST\",  \"method\" : \"sklearn\"}, \n                                               description='Predict MNIST with sklearn')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Deployment\n\nHet model hadden we al, dus we kunnen nu daadwerkelijk gaan deployen naar ACI. Ook hier weer even een pas op de plaats en weer een moment om collega's te helpen, het weer te bespreken, maar kijk uit met koffie halen, want je hebt al twee bakken op inmiddels. Het deployen van een container naar ACI kan zomaar 7 minuten duren. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.image import ContainerImage\n\n# configure the image\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\")\n\nservice = Webservice.deploy_from_model(workspace=ws,\n                                       name='sklearn-mnist-svc',\n                                       deployment_config=aciconfig,\n                                       models=[model],\n                                       image_config=image_config)\n\nservice.wait_for_deployment(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Feitelijk ben je nu klaar. Het model is getraind, getest en draait nu in een container in Azure. Neem wederom een moment om te realiseren wat je feitelijk hebt gedaan en hoe lang je er mee bezig bent geweest. \n\nUiteraard willen we ook de vruchten van ons werk kunnen plukken, dus willen we het endpoint van de webservice hebben:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(service.scoring_uri)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "...en als we toch de URL hebben, moeten we deze ook even testen:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\n\n# find 30 random samples from test set\nn = 30\nsample_indices = np.random.permutation(X_test.shape[0])[0:n]\n\ntest_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\ntest_samples = bytes(test_samples, encoding='utf8')\n\n# predict using the deployed model\nresult = service.run(input_data=test_samples)\n\n# compare actual value vs. the predicted values:\ni = 0\nplt.figure(figsize = (20, 1))\n\nfor s in sample_indices:\n    plt.subplot(1, n, i + 1)\n    plt.axhline('')\n    plt.axvline('')\n    \n    # use different color for misclassified sample\n    font_color = 'red' if y_test[s] != result[i] else 'black'\n    clr_map = plt.cm.gray if y_test[s] != result[i] else plt.cm.Greys\n    \n    plt.text(x=10, y =-10, s=result[i], fontsize=18, color=font_color)\n    plt.imshow(X_test[s].reshape(28, 28), cmap=clr_map)\n    \n    i = i + 1\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Cleanup\n\nDe resources die we hebben opgespind zijn geen goedkope resources om continu in je Azure subscription te hebben staan. RUIM DEZE DUS OP. Je kan ze altijd weer opspinnen middels dit script. Eventueel kan je natuurlijk de ACI laten staan, zodat je wel tegen de webservice kan blijven aanpraten."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Extra credit\n\nAls je al je collega's waar mogelijk hebt geholpen en je als Ã©chte IT'er al die conversatie ook wel zat ben, dan kan je altijd nog een .net app bouwen die de zojuist gemaakte endpoint consumeert. Denk aan een console app welke een image post naar het endpoint, of wellicht ga je echt los en bouw je een UWP app met een canvas waar je cijfers op kan tekenen... of ben jij heel handig in Xamarin en Unity en blaas je ons allemaal weg met een implementatie tijdens een toekomstige R&D dag."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Dank voor de aandacht\n![Awesome](https://ih0.redbubble.net/image.45552731.0363/flat,550x550,075,f.jpg \"You are awesome\")"
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "haining"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "msauthor": "haining"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}